# Advanced Conversation History Manager (`AdvancedConversationManager.ts`)

**Version:** 2.5.1

## üöÄ Overview

The `AdvancedConversationManager` is a sophisticated TypeScript module designed for intelligent management and selection of conversation history for Large Language Model (LLM) interactions. It resides client-side (e.g., within a Vue/React frontend) and aims to provide optimal, contextually relevant history segments to the LLM while adhering to configurable constraints like token limits.

This manager moves beyond simple recency-based truncation by incorporating:

* **NLP-Driven Relevance Scoring:** Uses the `natural` library (specifically Dice Coefficient for robust client-side string similarity after stemming and stopword removal) to assess how relevant past messages are to the current user query.
* **Ensemble of Strategies:** Implements multiple history selection strategies (Recency, Relevance-focused, Hybrid, Simple Recency) allowing for flexible adaptation to different conversational patterns and LLM requirements.
* **User-Configurable Presets:** Exposes user-friendly presets (e.g., "Balanced Hybrid," "Relevance Focused," "Concise Recent") that abstract complex underlying parameters.
* **Token Limit Adherence:** Strategies actively work to keep the selected history within a defined maximum token limit, using character-based estimation.

All logic, including interfaces, NLP utilities, strategies, and the manager class, is encapsulated within this single `AdvancedConversationManager.ts` file for ease of integration.

---

## ‚ú® How It Works

1.  **Configuration (`AdvancedHistoryConfig`):**
    * The manager maintains a reactive configuration object, persisted in `localStorage` via `@vueuse/core`'s `useStorage`.
    * Key configuration parameters include:
        * `strategyPreset`: The active preset determining the selection logic.
        * `maxContextTokens`: The target maximum token count for the selected history.
        * `relevancyThreshold`: Minimum score for a message to be considered relevant.
        * `numRecentMessagesToPrioritize`: Number of recent messages to give high priority.
        * `numRelevantOlderMessagesToInclude`: How many highly relevant older messages to fetch.
        * `simpleRecencyMessageCount`: For the basic "Simple Recency" preset.
        * `filterHistoricalSystemMessages`: Whether to strip past system messages.
        * `charsPerTokenEstimate`: For estimating token counts.

2.  **Message Preprocessing:**
    * Before relevance scoring or selection, messages (`ProcessedConversationMessage`) can be augmented:
        * `estimatedTokenCount`: Calculated using `charsPerTokenEstimate`.
        * `processedTokens`: Generated by tokenizing, stemming (PorterStemmer), and removing common English stopwords. This is used by the relevance scorer.

3.  **Relevance Scoring (`TfidfRelevanceScorer`):**
    * Given a current user query, this component scores historical messages.
    * It preprocesses the query and message content.
    * It primarily uses **Dice Coefficient** on the processed tokens for a balance of accuracy and client-side performance. While named `TfidfRelevanceScorer` for its initial TF-IDF aspiration, the practical client-side implementation defaults to Dice for robustness.

4.  **History Selection Strategies (`IHistorySelectionStrategy`):**
    * The manager dynamically chooses a strategy based on the `strategyPreset` in the configuration.
    * Available strategies:
        * `SimpleRecencyStrategy`: Takes the last `N` messages (configured by `simpleRecencyMessageCount`).
        * `RecencyStrategy`: Takes the last `N` messages (configured by `numRecentMessagesToPrioritize`).
        * `RelevanceStrategy`: Selects messages primarily based on their relevance score exceeding `relevancyThreshold`.
        * `HybridStrategy`: A sophisticated approach that first includes `numRecentMessagesToPrioritize` recent messages. Then, it scores older messages and includes up to `numRelevantOlderMessagesToInclude` that meet the `relevancyThreshold`.
    * All strategies ensure the final selected list is chronologically sorted and respects the `maxContextTokens` limit (system prompt token count is factored in).

5.  **`prepareHistoryForApi` Method:**
    * This is the main public method.
    * It takes the full session message history, the current user query text, and an optional system prompt text.
    * It ensures messages are preprocessed (token counts, NLP tokens).
    * It invokes the active strategy to get a selection of user/assistant messages.
    * The system prompt itself is not part of the returned array but its length is accounted for by the strategies when truncating by token limit. The calling code is expected to prepend the system prompt if needed.

---

## üõ†Ô∏è Configuration & Usage

### Initialization

The manager is exported as a singleton instance:

```typescript
import { advancedConversationManager } from './AdvancedConversationManager';

It initializes with default settings (see DEFAULT_ADVANCED_HISTORY_CONFIG) or loads existing settings from localStorage.
Configuration Methods

You can configure the manager programmatically, typically from a settings UI:

    Get Current Configuration:
    TypeScript

const currentConfig = advancedConversationManager.getHistoryConfig(); // Returns a read-only deep clone
console.log(currentConfig.strategyPreset, currentConfig.maxContextTokens);

Update Specific Configuration Values:
TypeScript

advancedConversationManager.updateConfig({
  maxContextTokens: 3500,
  relevancyThreshold: 0.3,
});

Changes are automatically persisted to localStorage.

Set Strategy via Presets:
Presets offer user-friendly ways to configure behavior without tweaking individual parameters.
TypeScript

import { HistoryStrategyPreset } from './AdvancedConversationManager';

// Get available presets for a UI dropdown
const presets = advancedConversationManager.getAvailablePresets();
// [
//   HistoryStrategyPreset.BALANCED_HYBRID,
//   HistoryStrategyPreset.RELEVANCE_FOCUSED,
//   ...
// ]

// Apply a preset
advancedConversationManager.setHistoryStrategyPreset(HistoryStrategyPreset.RELEVANCE_FOCUSED);
// This will update multiple related fields in the AdvancedHistoryConfig.

Available Presets:

    BALANCED_HYBRID (Default): Good mix of recency and relevance.
    RELEVANCE_FOCUSED: Prioritizes messages semantically similar to the current query.
    RECENT_CONVERSATION: Focuses on the latest part of the conversation.
    MAX_CONTEXT_HYBRID: Tries to fill a larger token window with recent and relevant messages.
    CONCISE_RECENT: For very short contexts, uses recent messages.
    SIMPLE_RECENCY: A basic strategy taking the last N messages, similar to non-advanced managers.

Get Default Configuration:
Useful for a "Reset to Defaults" button in settings.
TypeScript

    const defaults = advancedConversationManager.getDefaultConfig();
    // To apply: advancedConversationManager.updateConfig(defaults);
    // Or more simply, re-apply the default preset:
    // advancedConversationManager.setHistoryStrategyPreset(DEFAULT_ADVANCED_HISTORY_CONFIG.strategyPreset);

Preparing History for LLM

The primary use case is to get a curated list of messages:
TypeScript

import { advancedConversationManager, type ProcessedConversationMessage } from './AdvancedConversationManager';

async function getLlmReadyHistory(
  allMessagesInSession: ProcessedConversationMessage[],
  currentUserQuery: string,
  currentSystemPrompt: string
): Promise<ProcessedConversationMessage[]> {
  // `allMessagesInSession` should be the complete log of ProcessedConversationMessage objects
  // from your application's state (e.g., Pinia store, Vue component ref).

  const historyForLlm = await advancedConversationManager.prepareHistoryForApi(
    allMessagesInSession,
    currentUserQuery,
    currentSystemPrompt // System prompt text, its length is used for token budgeting
  );

  // `historyForLlm` is an array of user/assistant messages.
  // You would then typically prepend `currentSystemPrompt` before sending to the LLM.
  // Example:
  // const messagesToSend = [
  //   { role: 'system', content: currentSystemPrompt },
  //   ...historyForLlm
  // ];

  return historyForLlm;
}

Note on ProcessedConversationMessage:
The prepareHistoryForApi method expects messages to potentially have estimatedTokenCount and processedTokens. The manager's internal ensureMessagesAreProcessed method will attempt to populate these if missing on a clone of the input messages. For optimal performance, especially if allSessionMessages is very large and frequently processed, consider pre-calculating these fields when messages are first created or added to your main store.
Clearing History

The AdvancedConversationManager does not own or store the primary conversation message array. It only processes it. To clear the history:
TypeScript

function clearFullConversation(messageStore: any /* e.g., your Pinia store */) {
  advancedConversationManager.clearHistory(() => {
    // Your actual logic to clear messages from your store
    messageStore.deleteAllMessages();
    console.log("Message store cleared via callback.");
  });
}

üí° How it Differs from Regular Conversation Memory

Traditional/Simpler Conversation Memory typically:

    Relies on Recency Only: Keeps the last N messages or turns.
    Fixed Window: Uses a fixed number of messages or a simple character/token count for truncation.
    No Content Analysis: Doesn't analyze the content of messages for relevance to the current query. If a crucial piece of information was discussed 10 messages ago but the window is only 5 messages, it's lost.
    Less Adaptable: Often has a one-size-fits-all approach to history length.

The AdvancedConversationManager offers significant enhancements:

    Content-Aware Relevance (NLP): By scoring messages based on their semantic similarity (via Dice Coefficient on stemmed/tokenized content) to the current query, it can surface older messages that are highly relevant, even if they fall outside a simple recency window. This is crucial for conversations that revisit topics or have non-linear flows.
    Hybrid Strategies: The "Hybrid" strategies attempt to get the best of both worlds: maintain recent conversational flow and pull in contextually important older information.
    Dynamic & Configurable:
        Users (or the application logic) can switch strategies via presets based on the task or LLM capabilities (e.g., use CONCISE_RECENT for an LLM with a very small context window, or RELEVANCE_FOCUSED if the user is known to jump between topics).
        Fine-grained parameters (maxContextTokens, relevancyThreshold, etc.) allow for precise tuning.
    Improved Context Quality: The goal is not just to send some history, but the most useful history within the given constraints, potentially leading to more accurate and contextually appropriate LLM responses.
    Token Budgeting: More explicitly manages context window size by estimating token counts and truncating accordingly, helping to prevent API errors due to overly long prompts and manage costs.

Trade-offs:

    Client-Side Performance: NLP processing (even simplified) on the client-side adds computational overhead compared to just slicing an array. This implementation uses Dice Coefficient for a good performance/accuracy balance. For very large histories, performance should be monitored.
    Complexity: The system is inherently more complex than a simple recency buffer.
    natural Library Dependency: Adds a dependency to the frontend bundle.

This advanced manager is designed for applications where the quality of conversational context is paramount and providing the LLM with the most relevant historical information can significantly improve its performance and the user experience.