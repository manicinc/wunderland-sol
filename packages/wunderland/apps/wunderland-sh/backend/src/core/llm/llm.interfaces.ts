/**
 * @file Defines interfaces for Large Language Model (LLM) services and configurations.
 * @description This file specifies the contracts for interacting with different LLM providers,
 * managing their configurations, and handling responses in a standardized way.
 * It supports the architectural goal of provider flexibility (e.g., OpenAI, OpenRouter, Anthropic, Ollama).
 * @version 1.1.0 - Added support for tool/function calling.
 */

import { LlmProviderId } from "./llm.config.service.js"; // If LlmProviderId enum is to be used here

/**
 * @interface ILlmToolCallFunction
 * @description Describes the function called by the LLM as part of a tool call.
 */
export interface ILlmToolCallFunction {
  /** The name of the function to be called. */
  name: string;
  /**
   * The arguments to call the function with, as generated by the model in JSON format.
   * Note: The model does not always generate valid JSON, and may hallucinate parameters
   * not defined by your function schema. Validate the arguments in your code before calling
   * your function.
   */
  arguments: string; // JSON string
}

/**
 * @interface ILlmToolCall
 * @description Represents a tool call requested by the LLM.
 */
export interface ILlmToolCall {
  /** The ID of the tool call. This ID must be referenced when sending a response back to the model. */
  id: string;
  /** The type of the tool. Currently, only `function` is supported. */
  type: 'function';
  /** The function that the model wants to call. */
  function: ILlmToolCallFunction;
}

/**
 * Represents a single message in a chat conversation.
 * Aligns with common LLM API structures (e.g., OpenAI).
 */
export interface IChatMessage {
  /**
   * The role of the message sender.
   * 'system': Instructions or context for the AI.
   * 'user': Input from the end-user.
   * 'assistant': Responses generated by the AI.
   * 'tool': Response from a tool/function call.
   */
  role: 'system' | 'user' | 'assistant' | 'tool';

  /** The text content of the message. */
  content: string | null; // Can be null if tool_calls are present for assistant, or for tool role

  /**
   * Optional name of the participant.
   * Useful for multi-turn conversations or specifying function names in function calling scenarios.
   */
  name?: string;

  /**
   * Optional: The tool calls generated by the model, such as function calls.
   * This is typically present on an 'assistant' role message.
   */
  tool_calls?: ILlmToolCall[];

  /**
   * Optional: Tool call ID. This field is required if the role is `tool`.
   * If the role is `tool`, the `content` field contains the output of the tool call.
   * The `tool_call_id` must be the ID of the tool call from the 'assistant' message that requested this tool call.
   */
  tool_call_id?: string;
}

/**
 * Represents the token usage statistics returned by an LLM API call.
 */
export interface ILlmUsage {
  /** Number of tokens in the input prompt. Nullable if not provided by the API. */
  prompt_tokens: number | null;
  /** Number of tokens in the generated completion. Nullable if not provided by the API. */
  completion_tokens: number | null;
  /** Total tokens used (prompt + completion). Nullable if not provided by the API. */
  total_tokens: number | null;
}

/**
 * @interface ILlmFunctionParameterProperty
 * @description Defines a property within the parameters of a tool/function.
 */
export interface ILlmFunctionParameterProperty {
  type: "string" | "number" | "integer" | "boolean" | "array" | "object";
  description?: string;
  enum?: string[];
  items?: ILlmFunctionParameterProperty; // For type 'array'
  properties?: Record<string, ILlmFunctionParameterProperty>; // For type 'object'
  required?: string[]; // For type 'object'
}

/**
 * @interface ILlmToolFunctionParameters
 * @description Defines the parameters for a tool/function, following JSON Schema structure.
 */
export interface ILlmToolFunctionParameters {
  type: "object";
  properties: Record<string, ILlmFunctionParameterProperty>;
  required?: string[];
}

/**
 * @interface ILlmToolFunction
 * @description Defines a function that can be called by the LLM.
 */
export interface ILlmToolFunction {
  /** The name of the function to be called. */
  name: string;
  /** A description of what the function does, used by the model to choose when and how to call the function. */
  description: string;
  /** The parameters the functions accepts, described as a JSON Schema object. */
  parameters: ILlmToolFunctionParameters;
}

/**
 * @interface ILlmTool
 * @description Defines a tool that can be provided to the LLM.
 * Currently, only 'function' tools are supported.
 */
export interface ILlmTool {
  /** The type of the tool. Currently, only `function` is supported. */
  type: 'function';
  /** The function definition. */
  function: ILlmToolFunction;
}


/**
 * Represents a standardized response from an LLM service.
 */
export interface ILlmResponse {
  /** The primary text content of the AI's response. Null if the response is primarily tool calls or an error. */
  text: string | null;
  /** Identifier of the model that generated the response (e.g., "openai/gpt-4o-mini", "anthropic/claude-3-opus"). */
  model: string;
  /** Optional token usage statistics for the API call. */
  usage?: ILlmUsage;
  /** Optional unique identifier for the request or response chunk (if streaming), provided by the LLM. */
  id?: string;
  /**
   * Optional reason why the LLM stopped generating tokens.
   * Common values: "stop" (natural end), "length" (max_tokens limit), "tool_calls", "content_filter".
   */
  stopReason?: string;
  /**
   * Optional: Tool calls generated by the model.
   * If present, `text` might be null or contain preliminary remarks.
   */
  toolCalls?: ILlmToolCall[];
  /**
   * Any additional metadata or the raw, unparsed response from the provider.
   * Typed as 'unknown' for safety, requiring explicit type assertion if accessed.
   */
  providerResponse?: unknown;
}

/**
 * Configuration options for an LLM provider.
 */
export interface ILlmProviderConfig {
  /** Unique identifier for the provider (e.g., 'openai', 'openrouter', 'anthropic', 'ollama'). */
  providerId: LlmProviderId | string; // Using LlmProviderId enum or string for flexibility
  /** API key for the provider. Undefined for providers that don't require one (e.g., local Ollama). */
  apiKey: string | undefined;
  /** Base URL for the provider's API. */
  baseUrl?: string;
  /** Default model ID to use for this provider if not specified in a request. */
  defaultModel?: string;
  /** Optional additional HTTP headers to send with requests to this provider (e.g., for OpenRouter site identification). */
  additionalHeaders?: Record<string, string>;
}

/**
 * Common parameters for making a chat completion request to an LLM.
 */
export interface IChatCompletionParams {
  /**
   * Controls randomness. Lower values (e.g., 0.2) make output more focused and deterministic.
   * Higher values (e.g., 0.8) make it more random. Typically between 0.0 and 2.0.
   */
  temperature?: number;
  /** The maximum number of tokens to generate in the response. */
  max_tokens?: number;
  /**
   * Nucleus sampling parameter. The model considers tokens with top_p probability mass.
   * 0.1 means top 10% probability mass. Typically between 0.0 and 1.0.
   * OpenAI recommends altering only one of temperature or top_p.
   */
  top_p?: number;
  /** One or more sequences where the API will stop generating further tokens. */
  stop?: string | string[];
  /** A unique identifier for the end-user, aiding in monitoring and abuse detection. */
  user?: string;
  /**
   * If true, the response will be streamed. Handling streamed responses requires specific client-side logic.
   * This interface and associated services currently focus on non-streamed completions unless specified otherwise by a service.
   */
  stream?: boolean;
  /**
   * Number of chat completion choices to generate for each input message. Defaults to 1.
   */
  n?: number;
  /**
   * Positive values penalize new tokens based on whether they appear in the text so far,
   * increasing the model's likelihood to talk about new topics. Between -2.0 and 2.0.
   */
  presence_penalty?: number;
  /**
   * Positive values penalize new tokens based on their existing frequency in the text so far,
   * decreasing the model's likelihood to repeat the same line verbatim. Between -2.0 and 2.0.
   */
  frequency_penalty?: number;

  /**
   * A list of tools the model may call. Currently, only functions are supported as a tool.
   * Use this to provide a list of functions the model may generate JSON inputs for.
   */
  tools?: ILlmTool[];

  /**
   * Controls which (if any) function is called by the model.
   * `none` means the model will not call a function and will generate a message.
   * `auto` means the model can pick between generating a message or calling a function.
   * Specifying a particular function via `{"type": "function", "function": {"name": "my_function"}}`
   * forces the model to call that function.
   * `none` is the default when no functions are present. `auto` is the default if functions are present.
   */
  tool_choice?: 'none' | 'auto' | { type: 'function'; function: { name: string } };

  // Allow any other provider-specific parameters via index signature
  [key: string]: any;
}

/**
 * Interface for an LLM service.
 * Each concrete implementation (e.g., OpenAiLlmService, OpenRouterLlmService)
 * must adhere to this contract for standardized interaction.
 */
export interface ILlmService {
  /** The unique identifier of the LLM provider this service interacts with (e.g., "openai", "openrouter"). */
  readonly providerId: LlmProviderId | string;

  /**
   * Generates a chat completion based on the provided messages and model.
   * @param {IChatMessage[]} messages - An array of IChatMessage objects representing the conversation history and current prompt.
   * @param {string} modelId - The identifier of the LLM to use (e.g., "gpt-4o-mini", "anthropic/claude-3-opus").
   * @param {IChatCompletionParams} [params] - Optional parameters to customize the completion (e.g., temperature, max_tokens, tools).
   * @returns {Promise<ILlmResponse>} A Promise resolving to a standardized ILlmResponse, which may include text or tool calls.
   * @throws {Error} If the API call fails, configuration is missing, or the response cannot be processed.
   */
  generateChatCompletion(
    messages: IChatMessage[],
    modelId: string,
    params?: IChatCompletionParams
  ): Promise<ILlmResponse>;

  // Future methods could include:
  // generateEmbedding(texts: string[], modelId: string): Promise<{ embedding: number[]; usage: ILlmUsage }[]>;
  // listAvailableModels(): Promise<{ id: string; description?: string; contextWindow?: number }[]>;
}