// File: backend/agentos/core/orchestration/AgentOrchestrator.ts
/**
 * @fileoverview Implements the `AgentOSOrchestrator` class, the central nervous system for AgentOS interactions.
 * This module is responsible for the intricate coordination between the public-facing `AgentOS` API
 * and the cognitive core, the `GMI` (Generalized Mind Instance). It meticulously manages the
 * entire lifecycle of an interaction turn.
 *
 * Key Responsibilities:
 * - **GMI Instance Management**: Selection/creation of GMI instances via `GMIManager`.
 * - **Contextualization**: Preparation and loading of `ConversationContext`.
 * - **Input Processing**: Transformation of `AgentOSInput` into `GMITurnInput`.
 * - **Streaming Output Handling**: Iteration over `GMIOutputChunk` streams from the GMI.
 * - **Tool Use Coordination**:
 * - Relaying `ToolCallRequest`s to the client (via `StreamingManager`).
 * - Receiving tool execution results.
 * - Forwarding tool results back to the GMI via `handleToolResult`.
 * - Processing the GMI's response (`GMIOutput`) post-tool execution.
 * - **Response Streaming**: Conversion of GMI outputs into `AgentOSResponse` chunks and their
 * distribution through the `StreamingManager`.
 * - **State Management**: Tracking active interaction streams and their associated contexts.
 * - **Error Handling**: Robust error management for orchestration failures.
 *
 * The orchestrator embodies asynchronous processing and streaming, ensuring a responsive user experience.
 * It relies on dependency injection for its core components, promoting modularity and testability.
 *
 * @module backend/agentos/core/orchestration/AgentOSOrchestrator
 */

import { AgentOSInput, ProcessingOptions } from '../../api/types/AgentOSInput';
import {
  AgentOSResponse,
  AgentOSResponseChunkType,
  AgentOSTextDeltaChunk,
  AgentOSFinalResponseChunk,
  AgentOSErrorChunk,
  AgentOSSystemProgressChunk,
  AgentOSToolCallRequestChunk,
  AgentOSToolResultEmissionChunk,
  AgentOSUICommandChunk,
  AgentOSMetadataUpdateChunk,
  AgentOSWorkflowUpdateChunk,
  AgentOSAgencyUpdateChunk,
} from '../../api/types/AgentOSResponse';
import { GMIManager } from '../../cognitive_substrate/GMIManager';
import {
  IGMI,
  GMITurnInput,
  GMIOutputChunk,
  GMIOutput,
  ToolCallRequest,
  ToolResultPayload,
  GMIInteractionType,
  GMIOutputChunkType,
  UICommand,
  VisionInputData,
  AudioInputData,
} from '../../cognitive_substrate/IGMI';
import { ConversationManager } from '../conversation/ConversationManager';
import { ConversationContext } from '../conversation/ConversationContext';
import { ToolOrchestrator as ConcreteToolOrchestrator } from '../tools/ToolOrchestrator';
// import { ITool } from '../tools/ITool'; // ITool not directly used here, but good to keep if planned
import { MessageRole } from '../conversation/ConversationMessage';
import type { ConversationMessage } from '../conversation/ConversationMessage';
import { uuidv4 } from '@framers/agentos/utils/uuid';
import { GMIError, GMIErrorCode, createGMIErrorFromError } from '@framers/agentos/utils/errors';
import { StreamingManager, StreamId } from '../streaming/StreamingManager';
import { normalizeUsage, snapshotPersonaDetails } from './helpers';
import { AIModelProviderManager } from '../llm/providers/AIModelProviderManager';
import {
  DEFAULT_PROMPT_PROFILE_CONFIG,
  selectPromptProfile,
  type PromptProfileConfig,
  type PromptProfileConversationState,
} from '../prompting/PromptProfileRouter';
import {
  DEFAULT_ROLLING_SUMMARY_COMPACTION_CONFIG,
  maybeCompactConversationMessages,
  type RollingSummaryCompactionConfig,
  type RollingSummaryCompactionResult,
} from '../conversation/RollingSummaryCompactor';
import type { IRollingSummaryMemorySink, RollingSummaryMemoryUpdate } from '../conversation/IRollingSummaryMemorySink';
import {
  DEFAULT_LONG_TERM_MEMORY_POLICY,
  hasAnyLongTermMemoryScope,
  LONG_TERM_MEMORY_POLICY_METADATA_KEY,
  ORGANIZATION_ID_METADATA_KEY,
  resolveLongTermMemoryPolicy,
  type ResolvedLongTermMemoryPolicy,
} from '../conversation/LongTermMemoryPolicy';

export interface RollingSummaryCompactionProfileDefinition {
  config: RollingSummaryCompactionConfig;
  systemPrompt?: string;
}

export interface RollingSummaryCompactionProfilesConfig {
  defaultProfileId: string;
  defaultProfileByMode?: Record<string, string>;
  profiles: Record<string, RollingSummaryCompactionProfileDefinition>;
}

function normalizeMode(value: string): string {
  return (value || '').trim().toLowerCase();
}

function pickByMode(map: Record<string, string> | undefined, mode: string): string | null {
  if (!map || Object.keys(map).length === 0) return null;
  const modeNorm = normalizeMode(mode);
  const exact = map[modeNorm];
  if (exact) return exact;
  const patternMatch = Object.entries(map)
    .map(([key, value]) => ({ key: normalizeMode(key), value }))
    .filter(({ key }) => key && (modeNorm === key || modeNorm.startsWith(key) || modeNorm.includes(key)))
    .sort((a, b) => b.key.length - a.key.length)[0];
  return patternMatch?.value ?? null;
}

/**
 * Configuration options for the AgentOSOrchestrator.
 * These settings govern the behavior and limits of the orchestration process.
 * @interface AgentOSOrchestratorConfig
 */
export interface AgentOSOrchestratorConfig {
  /**
   * The maximum number of sequential tool call iterations allowed within a single logical turn.
   * This prevents potential infinite loops if GMIs repeatedly call tools without resolution.
   * @type {number}
   * @default 5
   */
  maxToolCallIterations?: number;

  /**
   * Default timeout in milliseconds for a single GMI processing step
   * (e.g., initial turn or processing after a tool result).
   * Note: Actual timeout implementation might reside within the GMI or LLM provider layer.
   * @type {number}
   * @default 120000 (2 minutes)
   */
  defaultAgentTurnTimeoutMs?: number;

  /**
   * If true, conversation context will be persistently saved and loaded by the `ConversationManager`.
   * If false, conversations are primarily in-memory, and their persistence depends on the
   * `ConversationManager`'s specific configuration.
   * @type {boolean}
   * @default true
   */
  enableConversationalPersistence?: boolean;
  /** If true, orchestrator logs detailed information about tool calls. */
  logToolCalls?: boolean;

  /**
   * Optional prompt-profile routing config. If omitted, a small default router is used.
   * Set to `null` to disable prompt-profile routing entirely.
   */
  promptProfileConfig?: PromptProfileConfig | null;

  /**
   * Optional rolling-summary compaction config. If omitted, a conservative default is used (disabled).
   * Set to `null` to disable rolling-summary compaction entirely.
   */
  rollingSummaryCompactionConfig?: RollingSummaryCompactionConfig | null;

  /**
   * Optional rolling-summary compaction profiles. When provided, the orchestrator selects a compaction
   * profile per-turn based on `mode` (customFlags.mode or persona id) and uses it instead of the
   * single `rollingSummaryCompactionConfig`.
   */
  rollingSummaryCompactionProfilesConfig?: RollingSummaryCompactionProfilesConfig | null;

  /** Optional system prompt override for rolling-summary compaction. */
  rollingSummarySystemPrompt?: string;

  /** Optional metadata key to store rolling-summary state under (defaults to `rollingSummaryState`). */
  rollingSummaryStateKey?: string;
}

/**
 * Defines the dependencies required by the AgentOSOrchestrator for its operation.
 * These services are typically injected during the orchestrator's initialization phase,
 * adhering to the Dependency Inversion Principle.
 * @interface AgentOSOrchestratorDependencies
 */
export interface AgentOSOrchestratorDependencies {
  /** An instance of GMIManager for managing GMI instances and persona definitions. */
  gmiManager: GMIManager;
  /** An instance of ToolOrchestrator for orchestrating the execution of tools. */
  toolOrchestrator: ConcreteToolOrchestrator;
  /** An instance of ConversationManager for managing conversation contexts. */
  conversationManager: ConversationManager;
  /** An instance of StreamingManager for managing streaming responses to clients. */
  streamingManager: StreamingManager;
  /** AI model provider manager used for internal routing/compaction tasks. */
  modelProviderManager: AIModelProviderManager;
  /**
   * Optional sink for persisting rolling-memory outputs (`summary_markdown` + `memory_json`)
   * into a long-term store (RAG / knowledge graph / database).
   */
  rollingSummaryMemorySink?: IRollingSummaryMemorySink;
}

/**
 * Internal state for managing an active stream of GMI interaction, associated with an `agentOSStreamId`.
 * This context bundles all necessary information and instances for processing a single user interaction flow.
 * @interface ActiveStreamContext
 * @private
 */
interface ActiveStreamContext {
  /** The active GMI instance for this stream. */
  gmi: IGMI;
  /** The ID of the user initiating the interaction. */
  userId: string;
  /** The AgentOS session ID for this interaction. */
  sessionId: string;
  /** The ID of the currently active persona in the GMI. */
  personaId: string;
  /** The specific ID of the conversation context being used. */
  conversationId: string;
  /** The live conversation context object. */
  conversationContext: ConversationContext;
  /** Optional user-provided API keys for external services. */
  userApiKeys?: Record<string, string>;
  /** Optional processing options for the current turn. */
  processingOptions?: ProcessingOptions;
}

/**
 * @class AgentOSOrchestrator
 * @classdesc
 * The `AgentOSOrchestrator` serves as the master conductor of interactions within the AgentOS platform.
 * It is the critical intermediary layer that bridges high-level API requests (`AgentOSInput`)
 * with the sophisticated cognitive processing of Generalized Mind Instances (`IGMI`).
 * Its primary function is to manage the entire lifecycle of a user's interaction turn, a potentially
 * complex, multi-step process.
 */
export class AgentOSOrchestrator {
  private initialized: boolean = false;
  private config!: Readonly<Required<AgentOSOrchestratorConfig>>;
  private dependencies!: Readonly<AgentOSOrchestratorDependencies>;
  
  /**
   * Stores the context for active, ongoing interaction streams.
   * The key is the `agentOSStreamId` generated by this orchestrator.
   * @private
   * @type {Map<StreamId, ActiveStreamContext>}
   */
  private readonly activeStreamContexts: Map<StreamId, ActiveStreamContext>;

  /**
   * Constructs an AgentOSOrchestrator instance.
   * The orchestrator is not operational until `initialize()` is successfully called.
   */
  constructor() {
    this.activeStreamContexts = new Map();
  }

  /**
   * Initializes the AgentOSOrchestrator with its configuration and essential service dependencies.
   * This method must be called and resolve successfully before the orchestrator can process any requests.
   * It sets up the orchestrator's internal state and validates its dependencies.
   *
   * @public
   * @async
   * @param {AgentOSOrchestratorConfig} config - Configuration settings for the orchestrator.
   * @param {AgentOSOrchestratorDependencies} dependencies - An object containing instances of required services.
   * @returns {Promise<void>} A Promise that resolves when initialization is complete.
   * @throws {GMIError} If critical dependencies are missing (GMIErrorCode.CONFIGURATION_ERROR).
   */
  public async initialize(
    config: AgentOSOrchestratorConfig,
    dependencies: AgentOSOrchestratorDependencies,
  ): Promise<void> {
    if (this.initialized) {
      console.warn('AgentOSOrchestrator: Instance is already initialized. Skipping re-initialization.');
      return;
    }

    if (!dependencies.gmiManager || !dependencies.toolOrchestrator || !dependencies.conversationManager || !dependencies.streamingManager || !dependencies.modelProviderManager) {
      const missingDeps = [
        !dependencies.gmiManager && "gmiManager",
        !dependencies.toolOrchestrator && "toolOrchestrator",
        !dependencies.conversationManager && "conversationManager",
        !dependencies.streamingManager && "streamingManager",
        !dependencies.modelProviderManager && "modelProviderManager",
      ].filter(Boolean).join(', ');
      throw new GMIError(
        `AgentOSOrchestrator: Initialization failed due to missing essential dependencies: ${missingDeps}.`,
        GMIErrorCode.CONFIGURATION_ERROR,
        { missingDependencies: missingDeps }
      );
    }

    this.config = Object.freeze({
      maxToolCallIterations: config.maxToolCallIterations ?? 5,
      defaultAgentTurnTimeoutMs: config.defaultAgentTurnTimeoutMs ?? 120000,
      enableConversationalPersistence: config.enableConversationalPersistence ?? true,
      logToolCalls: config.logToolCalls ?? false,
      promptProfileConfig: config.promptProfileConfig === null
        ? null
        : (config.promptProfileConfig ?? DEFAULT_PROMPT_PROFILE_CONFIG),
      rollingSummaryCompactionConfig: config.rollingSummaryCompactionConfig === null
        ? null
        : { ...DEFAULT_ROLLING_SUMMARY_COMPACTION_CONFIG, ...(config.rollingSummaryCompactionConfig ?? {}) },
      rollingSummaryCompactionProfilesConfig: config.rollingSummaryCompactionProfilesConfig ?? null,
      rollingSummarySystemPrompt: config.rollingSummarySystemPrompt ?? '',
      rollingSummaryStateKey: config.rollingSummaryStateKey ?? 'rollingSummaryState',
    });
    this.dependencies = Object.freeze(dependencies);
    this.initialized = true;
    console.log(`AgentOSOrchestrator initialized successfully. Configuration: ${JSON.stringify(this.config)}`);
  }

  /**
   * Ensures that the orchestrator has been initialized before performing operations.
   * @private
   * @throws {GMIError} If the orchestrator is not initialized (GMIErrorCode.NOT_INITIALIZED).
   */
  private ensureInitialized(): void {
    if (!this.initialized) {
      throw new GMIError(
        'AgentOSOrchestrator is not initialized. Please call the initialize() method first.',
        GMIErrorCode.NOT_INITIALIZED,
        { component: 'AgentOSOrchestrator' }
      );
    }
  }

  /**
   * Pushes a response chunk to the client via the StreamingManager.
   * This is a centralized helper for all chunk types.
   * @private
   * @param {StreamId} agentOSStreamId - The unique ID of the AgentOS stream.
   * @param {AgentOSResponseChunkType} type - The type of chunk being pushed.
   * @param {string} gmiInstanceId - The ID of the GMI instance generating this chunk.
   * @param {string} personaId - The ID of the persona active during this chunk's generation.
   * @param {boolean} isFinal - Whether this chunk marks the end of the response for this stream.
   * @param {any} data - The specific payload data for the chunk, varying by `type`.
   * @returns {Promise<void>}
   */
  private async pushChunkToStream(
    agentOSStreamId: StreamId,
    type: AgentOSResponseChunkType,
    gmiInstanceId: string,
    personaId: string, // This is now consistently a string
    isFinal: boolean,
    data: any
  ): Promise<void> {
    const baseChunk = { type, streamId: agentOSStreamId, gmiInstanceId, personaId, isFinal, timestamp: new Date().toISOString() };
    let chunk: AgentOSResponse;

    switch (type) {
      case AgentOSResponseChunkType.TEXT_DELTA:
        chunk = { ...baseChunk, textDelta: data.textDelta } as AgentOSTextDeltaChunk;
        break;
      case AgentOSResponseChunkType.SYSTEM_PROGRESS:
        chunk = { ...baseChunk, message: data.message, progressPercentage: data.progressPercentage, statusCode: data.statusCode } as AgentOSSystemProgressChunk;
        break;
      case AgentOSResponseChunkType.TOOL_CALL_REQUEST:
        chunk = { ...baseChunk, toolCalls: data.toolCalls, rationale: data.rationale } as AgentOSToolCallRequestChunk;
        break;
      case AgentOSResponseChunkType.TOOL_RESULT_EMISSION:
        chunk = { ...baseChunk, toolCallId: data.toolCallId, toolName: data.toolName, toolResult: data.toolResult, isSuccess: data.isSuccess, errorMessage: data.errorMessage } as AgentOSToolResultEmissionChunk;
        break;
      case AgentOSResponseChunkType.UI_COMMAND:
        chunk = { ...baseChunk, uiCommands: data.uiCommands } as AgentOSUICommandChunk;
        break;
      case AgentOSResponseChunkType.ERROR:
        chunk = { ...baseChunk, code: data.code, message: data.message, details: data.details } as AgentOSErrorChunk;
        break;
      case AgentOSResponseChunkType.FINAL_RESPONSE:
        chunk = { 
          ...baseChunk, 
          finalResponseText: data.finalResponseText,
          finalToolCalls: data.finalToolCalls,
          finalUiCommands: data.finalUiCommands,
          audioOutput: data.audioOutput,
          imageOutput: data.imageOutput,
          usage: normalizeUsage(data.usage),
          reasoningTrace: data.reasoningTrace,
          error: data.error,
          updatedConversationContext: data.updatedConversationContext,
          activePersonaDetails: data.activePersonaDetails
        } as AgentOSFinalResponseChunk;
        break;
      case AgentOSResponseChunkType.METADATA_UPDATE:
        chunk = { ...baseChunk, updates: data.updates } as AgentOSMetadataUpdateChunk;
        break;
      case AgentOSResponseChunkType.WORKFLOW_UPDATE:
        chunk = { ...baseChunk, workflow: data.workflow } as AgentOSWorkflowUpdateChunk;
        break;
      case AgentOSResponseChunkType.AGENCY_UPDATE:
        chunk = { ...baseChunk, agency: data.agency } as AgentOSAgencyUpdateChunk;
        break;
      case AgentOSResponseChunkType.PROVENANCE_EVENT:
        chunk = { ...baseChunk, ...data } as any;
        break;
      default: {
        const exhaustiveCheck: never = type;
        const errorMsg = `Internal Error: Unknown stream chunk type '${exhaustiveCheck as string}' encountered by orchestrator.`;
        console.error(`AgentOSOrchestrator: ${errorMsg}`);
        chunk = { 
          ...baseChunk, 
          type: AgentOSResponseChunkType.ERROR, 
          code: GMIErrorCode.INTERNAL_SERVER_ERROR.toString(), 
          message: errorMsg,
          details: { originalData: data, originalType: type as string }
        } as AgentOSErrorChunk;
        break;
      }
    }
    try {
      await this.dependencies.streamingManager.pushChunk(agentOSStreamId, chunk);
    } catch (pushError: any) {
      console.error(`AgentOSOrchestrator: Failed to push chunk to stream ${agentOSStreamId}. Type: ${type}. Error: ${pushError.message}`, pushError);
    }
  }

  /**
   * Pushes a standardized error chunk to the client.
   * @private
   * @param {StreamId} agentOSStreamId - The ID of the AgentOS stream.
   * @param {string} personaId - The ID of the persona associated with this error.
   * @param {string} [gmiInstanceId='unknown_gmi_instance'] - The ID of the GMI instance.
   * @param {GMIErrorCode | string} code - The error code.
   * @param {string} message - The error message.
   * @param {any} [details] - Additional error details.
   * @returns {Promise<void>}
   */
  private async pushErrorChunk(
    agentOSStreamId: StreamId,
    personaId: string, // Ensured to be string by callers
    gmiInstanceId: string = 'unknown_gmi_instance',
    code: GMIErrorCode | string,
    message: string,
    details?: any
  ): Promise<void> {
    await this.pushChunkToStream(
      agentOSStreamId, AgentOSResponseChunkType.ERROR,
      gmiInstanceId, personaId, true, 
      { code: code.toString(), message, details }
    );
  }
  
  /**
   * Orchestrates a complete user interaction turn, starting from an `AgentOSInput`.
   * This method initiates a new stream for the interaction via `StreamingManager`,
   * then delegates the complex, potentially long-running processing to `_processTurnInternal`
   * without awaiting its completion. This allows `orchestrateTurn` to return the `StreamId`
   * to the caller (e.g., `AgentOS` facade) immediately, enabling clients to subscribe to the
   * stream for real-time updates.
   *
   * @public
   * @async
   * @param {AgentOSInput} input - The comprehensive input for the current turn from the API layer.
   * @returns {Promise<StreamId>} The unique ID of the stream established for this interaction.
   * Clients should use this ID to listen for `AgentOSResponse` chunks.
   * @throws {GMIError} If the `StreamingManager` fails to create a new stream (e.g., `GMIErrorCode.STREAM_ERROR`).
   */
  public async orchestrateTurn(input: AgentOSInput): Promise<StreamId> {
    this.ensureInitialized();
    const baseStreamId = input.sessionId ? `agentos-session-${input.sessionId}` : `agentos-turn-${uuidv4()}`;
    const agentOSStreamId = await this.dependencies.streamingManager.createStream(baseStreamId);
    
    console.log(`AgentOSOrchestrator: Orchestrating new turn. AgentOS Stream ID: ${agentOSStreamId}, User: ${input.userId}, Input Session: ${input.sessionId}`);

    this._processTurnInternal(agentOSStreamId, input).catch(async (criticalError: any) => {
      console.error(`AgentOSOrchestrator: CRITICAL UNHANDLED error from _processTurnInternal initiation for stream ${agentOSStreamId}:`, criticalError);
      const streamContext = this.activeStreamContexts.get(agentOSStreamId);
      const personaIdForError = input.selectedPersonaId || streamContext?.personaId || 'unknown_critical_error_persona';
      const gmiInstanceIdForError = streamContext?.gmi?.getGMIId() || 'orchestrator_pre_gmi_critical_error'; 
      try {
        await this.pushErrorChunk(
          agentOSStreamId, personaIdForError, gmiInstanceIdForError,
          GMIErrorCode.INTERNAL_SERVER_ERROR,
          `A critical unrecoverable orchestration error occurred: ${criticalError.message}`,
          { errorName: criticalError.name, rawErrorString: String(criticalError), stack: criticalError.stack }
        );
        if (this.activeStreamContexts.has(agentOSStreamId)) {
            // TS2554 fix: Pass only 1 or 2 arguments
          await this.dependencies.streamingManager.closeStream(agentOSStreamId, "Critical orchestrator error during turn processing initiation."); 
        }
      } catch (cleanupError: any) {
        console.error(`AgentOSOrchestrator: Error during critical error cleanup messaging for stream ${agentOSStreamId}:`, cleanupError);
      }
      this.activeStreamContexts.delete(agentOSStreamId);
    });

    return agentOSStreamId;
  }
  
  /**
   * Constructs a `GMITurnInput` object from the API-level `AgentOSInput`.
   * This method maps and transforms data to the structure expected by the `IGMI`.
   *
   * @private
   * @param {StreamId} agentOSStreamId - The orchestrator-level stream ID, used to generate a GMI interaction ID.
   * @param {AgentOSInput} agentOSInput - The input received from the AgentOS API layer.
   * @param {string} activePersonaId - The definitive ID of the persona resolved for this turn. Guaranteed to be a string by caller.
   * @returns {GMITurnInput} The structured input object ready for the GMI.
   */
  private constructGMITurnInput(agentOSStreamId: StreamId, agentOSInput: AgentOSInput, activePersonaId: string): GMITurnInput { // activePersonaId is string
    const { userId, sessionId, options, textInput, visionInputs, audioInput, userFeedback, selectedPersonaId, userApiKeys } = agentOSInput;

    const gmiInputMetadata: GMITurnInput['metadata'] = {
      options: {
        ...(options || {}),
        preferredModelId: options?.preferredModelId,
        preferredProviderId: options?.preferredProviderId,
        temperature: options?.temperature,
        topP: options?.topP,
        maxTokens: options?.maxTokens,
        toolChoice: (options as any)?.toolChoice, 
        responseFormat: (options as any)?.responseFormat,
      },
      userApiKeys,
      userFeedback,
      explicitPersonaSwitchId: selectedPersonaId !== activePersonaId ? selectedPersonaId : undefined,
      taskHint: '', 
    };

    let type: GMIInteractionType;
    let content: GMITurnInput['content'];

    if ((visionInputs && visionInputs.length > 0) || audioInput) {
      type = GMIInteractionType.MULTIMODAL_CONTENT;
      const multiModalPayload: { text?: string | null; vision?: VisionInputData[]; audio?: AudioInputData } = {};
      if (textInput !== null && textInput !== undefined) multiModalPayload.text = textInput;
      if (visionInputs) multiModalPayload.vision = visionInputs;
      if (audioInput) multiModalPayload.audio = audioInput;
      content = multiModalPayload;
      if (gmiInputMetadata) gmiInputMetadata.taskHint = textInput ? 'user_multimodal_query_with_text' : (visionInputs && visionInputs.length > 0 && audioInput) ? 'user_visual_audio_query' : (visionInputs && visionInputs.length > 0) ? 'user_visual_query' : 'user_audio_query';
    } else if (textInput !== null && textInput !== undefined && textInput.trim() !== '') {
      type = GMIInteractionType.TEXT;
      content = textInput;
      if (gmiInputMetadata) gmiInputMetadata.taskHint = 'user_text_query';
    } else if (userFeedback) {
      type = GMIInteractionType.SYSTEM_MESSAGE;
      content = { type: 'feedback_provided', payload: userFeedback };
      if (gmiInputMetadata) gmiInputMetadata.taskHint = 'user_feedback_submission';
    } else {
      type = GMIInteractionType.SYSTEM_MESSAGE;
      content = "System: Orchestrator initiating turn with no direct textual, visual, or audio input from user.";
      if (gmiInputMetadata) gmiInputMetadata.taskHint = 'system_initiated_turn_or_ping';
      console.warn(`AgentOSOrchestrator (Stream: ${agentOSStreamId}): No primary user input. Classifying as system message.`);
    }
    
    return {
      interactionId: `${agentOSStreamId}_gmi_turn_${uuidv4()}`,
      userId,
      sessionId: sessionId || agentOSStreamId,
      type,
      content,
      metadata: gmiInputMetadata,
      timestamp: new Date(),
    };
  }

  /**
   * Core internal turn processing logic. This method is executed asynchronously by `orchestrateTurn`.
   * It manages the GMI interaction loop, including streaming chunks, handling tool call requests,
   * and ensuring finalization of the stream.
   *
   * @private
   * @async
   * @param {StreamId} agentOSStreamId - The unique ID for this AgentOS-level stream.
   * @param {AgentOSInput} input - The initial input for this turn.
   * @returns {Promise<void>} Resolves when processing for this turn is complete or a terminal error occurs.
   */
  private async _processTurnInternal(agentOSStreamId: StreamId, input: AgentOSInput): Promise<void> {
    let gmi: IGMI | undefined;
    let conversationContext: ConversationContext | undefined;
    const requestedPersonaId = input.selectedPersonaId || 'default';
    let activePersonaId: string = requestedPersonaId;
    let gmiInstanceIdForChunks = 'gmi_pending_initialization';
    let activeStreamCtx: ActiveStreamContext | undefined;
    let organizationIdForMemory: string | undefined;
    let longTermMemoryPolicy: ResolvedLongTermMemoryPolicy | null = null;

    try {
      const gmiResult = await this.dependencies.gmiManager.getOrCreateGMIForSession(
        input.userId,
        input.sessionId || agentOSStreamId,
        requestedPersonaId, 
        input.conversationId
      );
      gmi = gmiResult.gmi;
      conversationContext = gmiResult.conversationContext;
      activePersonaId = gmi.getCurrentPrimaryPersonaId(); // Ensure this returns a string
      gmiInstanceIdForChunks = gmi.getGMIId();

      activeStreamCtx = {
        gmi, userId: input.userId, sessionId: input.sessionId || agentOSStreamId, personaId: activePersonaId,
        conversationId: conversationContext.sessionId, 
        conversationContext, userApiKeys: input.userApiKeys, processingOptions: input.options
      };
      this.activeStreamContexts.set(agentOSStreamId, activeStreamCtx);

      await this.pushChunkToStream(
        agentOSStreamId, AgentOSResponseChunkType.SYSTEM_PROGRESS,
        gmiInstanceIdForChunks, activePersonaId, false,
        { message: `Persona ${activePersonaId} ready. GMI Instance: ${gmiInstanceIdForChunks}`, progressPercentage: 10 }
      );

      const gmiTurnInput = this.constructGMITurnInput(agentOSStreamId, input, activePersonaId);

      // --- Org context + long-term memory policy (persisted per conversation) ---
      if (conversationContext) {
        const inboundOrg =
          typeof input.organizationId === 'string' ? input.organizationId.trim() : '';
        const storedOrgRaw = conversationContext.getMetadata(ORGANIZATION_ID_METADATA_KEY);
        const storedOrg =
          typeof storedOrgRaw === 'string' ? storedOrgRaw.trim() : '';

        organizationIdForMemory = inboundOrg || storedOrg || undefined;
        if (organizationIdForMemory && organizationIdForMemory !== storedOrg) {
          conversationContext.setMetadata(ORGANIZATION_ID_METADATA_KEY, organizationIdForMemory);
        }

        const rawPrevPolicy = conversationContext.getMetadata(LONG_TERM_MEMORY_POLICY_METADATA_KEY);
        const prevPolicy =
          rawPrevPolicy && typeof rawPrevPolicy === 'object'
            ? (rawPrevPolicy as ResolvedLongTermMemoryPolicy)
            : null;
        const inputPolicy = input.memoryControl?.longTermMemory ?? null;

        longTermMemoryPolicy = resolveLongTermMemoryPolicy({
          previous: prevPolicy,
          input: inputPolicy,
          defaults: DEFAULT_LONG_TERM_MEMORY_POLICY,
        });

        if (inputPolicy || !prevPolicy) {
          conversationContext.setMetadata(LONG_TERM_MEMORY_POLICY_METADATA_KEY, longTermMemoryPolicy);
        }
      } else {
        organizationIdForMemory =
          typeof input.organizationId === 'string' ? input.organizationId.trim() : undefined;
        longTermMemoryPolicy = resolveLongTermMemoryPolicy({
          defaults: DEFAULT_LONG_TERM_MEMORY_POLICY,
        });
      }

      (gmiTurnInput.metadata ??= {} as any).organizationId = organizationIdForMemory ?? null;
      (gmiTurnInput.metadata as any).longTermMemoryPolicy = longTermMemoryPolicy;

      // Persist the current user turn into the shared ConversationContext so persona switches and restarts
      // preserve conversation memory. This is intentionally done before the LLM call so failures still
      // retain the user's input.
      if (this.config.enableConversationalPersistence && conversationContext) {
        try {
          if (gmiTurnInput.type === GMIInteractionType.TEXT && typeof gmiTurnInput.content === 'string') {
            conversationContext.addMessage({
              role: MessageRole.USER,
              content: gmiTurnInput.content,
              name: input.userId,
              metadata: { agentPersonaId: activePersonaId, source: 'agentos_input' },
            });
          } else if (gmiTurnInput.type === GMIInteractionType.MULTIMODAL_CONTENT) {
            conversationContext.addMessage({
              role: MessageRole.USER,
              content: JSON.stringify(gmiTurnInput.content),
              name: input.userId,
              metadata: { agentPersonaId: activePersonaId, source: 'agentos_input_multimodal' },
            });
          } else if (gmiTurnInput.type === GMIInteractionType.SYSTEM_MESSAGE) {
            conversationContext.addMessage({
              role: MessageRole.SYSTEM,
              content: typeof gmiTurnInput.content === 'string' ? gmiTurnInput.content : JSON.stringify(gmiTurnInput.content),
              metadata: { agentPersonaId: activePersonaId, source: 'agentos_input_system' },
            });
          }
          await this.dependencies.conversationManager.saveConversation(conversationContext);
        } catch (error: any) {
          console.warn(
            `AgentOSOrchestrator (Stream: ${agentOSStreamId}): Failed to persist inbound message to ConversationContext.`,
            error,
          );
        }
      }

      // Build conversationHistoryForPrompt after compaction/routing so it can reflect rolling-summary trimming.

      const modeForRouting =
        typeof input.options?.customFlags?.mode === 'string' && input.options.customFlags.mode.trim()
          ? input.options.customFlags.mode.trim()
          : activePersonaId;

      // --- Rolling summary compaction (text + JSON metadata) ---
      let rollingSummaryResult: RollingSummaryCompactionResult | null = null;
      let rollingSummaryProfileId: string | null = null;
      let rollingSummaryConfigForTurn: RollingSummaryCompactionConfig | null = this.config.rollingSummaryCompactionConfig;
      let rollingSummarySystemPromptForTurn: string | undefined = this.config.rollingSummarySystemPrompt;

      if (this.config.rollingSummaryCompactionProfilesConfig) {
        const profilesConfig = this.config.rollingSummaryCompactionProfilesConfig;
        const picked =
          pickByMode(profilesConfig.defaultProfileByMode, modeForRouting) ??
          profilesConfig.defaultProfileId;
        rollingSummaryProfileId = picked;
        const profile = profilesConfig.profiles?.[picked];
        if (profile?.config) {
          rollingSummaryConfigForTurn = profile.config;
        }
        if (profile?.systemPrompt) {
          rollingSummarySystemPromptForTurn = profile.systemPrompt;
        }
      }

      if (conversationContext && rollingSummaryConfigForTurn) {
        try {
          const llmCaller = async (call: {
            providerId?: string;
            modelId: string;
            messages: any[];
            options: any;
          }): Promise<string> => {
            const providerIdResolved =
              call.providerId ||
              this.dependencies.modelProviderManager.getProviderForModel(call.modelId)?.providerId ||
              this.dependencies.modelProviderManager.getDefaultProvider()?.providerId;
            if (!providerIdResolved) {
              throw new Error(`No provider resolved for rolling-summary model '${call.modelId}'.`);
            }
            const provider = this.dependencies.modelProviderManager.getProvider(providerIdResolved);
            if (!provider) {
              throw new Error(`Provider '${providerIdResolved}' not found for rolling-summary compaction.`);
            }
            const response = await provider.generateCompletion(call.modelId, call.messages, call.options);
            const choice = response?.choices?.[0];
            const content = choice?.message?.content ?? choice?.text ?? '';
            if (typeof content === 'string') return content.trim();
            if (Array.isArray(content)) {
              return content
                .map((part: any) => (typeof part?.text === 'string' ? part.text : ''))
                .filter(Boolean)
                .join('\n')
                .trim();
            }
            return String(content ?? '').trim();
          };

          const stateKey = this.config.rollingSummaryStateKey;
          const compaction = await maybeCompactConversationMessages({
            messages: conversationContext.getAllMessages() as any,
            sessionMetadata: conversationContext.getAllMetadata() as any,
            config: rollingSummaryConfigForTurn,
            llmCaller: ({ providerId, modelId, messages, options }) =>
              llmCaller({ providerId, modelId, messages, options }),
            systemPrompt: rollingSummarySystemPromptForTurn,
            stateKey,
          });

          rollingSummaryResult = compaction;
          if (compaction.updatedSessionMetadata && Object.prototype.hasOwnProperty.call(compaction.updatedSessionMetadata, stateKey)) {
            conversationContext.setMetadata(stateKey, (compaction.updatedSessionMetadata as any)[stateKey]);
          }
        } catch (error: any) {
          console.warn(
            `AgentOSOrchestrator (Stream: ${agentOSStreamId}): Rolling summary compaction failed (continuing without it).`,
            error,
          );
        }
      }

      const turnMetadata = (gmiTurnInput.metadata ??= {} as any);
      const rollingSummaryEnabled = Boolean(rollingSummaryConfigForTurn?.enabled);
      const rollingSummaryText =
        rollingSummaryEnabled && typeof rollingSummaryResult?.summaryText === 'string'
          ? rollingSummaryResult.summaryText.trim()
          : '';
      turnMetadata.rollingSummary =
        rollingSummaryEnabled && rollingSummaryText
          ? { text: rollingSummaryText, json: rollingSummaryResult?.summaryJson ?? undefined }
          : null;

      // --- Prompt-profile routing (concise/deep/planner/reviewer) ---
      let promptProfileSelection: ReturnType<typeof selectPromptProfile>['result'] | null = null;
      if (conversationContext && this.config.promptProfileConfig) {
        try {
          const rawPrev = conversationContext.getMetadata('promptProfileState');
          const previousState: PromptProfileConversationState | null =
            rawPrev && typeof rawPrev === 'object' && typeof (rawPrev as any).presetId === 'string'
              ? (rawPrev as PromptProfileConversationState)
              : null;

          const userMessageForRouting =
            gmiTurnInput.type === GMIInteractionType.TEXT && typeof gmiTurnInput.content === 'string'
              ? gmiTurnInput.content
              : gmiTurnInput.type === GMIInteractionType.MULTIMODAL_CONTENT
                ? JSON.stringify(gmiTurnInput.content)
                : '';

          const selection = selectPromptProfile(
            this.config.promptProfileConfig,
            {
              conversationId: conversationContext.sessionId,
              mode: modeForRouting,
              userMessage: userMessageForRouting,
              didCompact: Boolean(rollingSummaryResult?.didCompact),
            },
            previousState,
          );
          promptProfileSelection = selection.result;
          conversationContext.setMetadata('promptProfileState', selection.nextState);
        } catch (error: any) {
          console.warn(
            `AgentOSOrchestrator (Stream: ${agentOSStreamId}): Prompt-profile routing failed (continuing without it).`,
            error,
          );
        }
      }

      turnMetadata.promptProfile = promptProfileSelection
        ? {
            id: promptProfileSelection.presetId,
            systemInstructions: promptProfileSelection.systemInstructions,
            reason: promptProfileSelection.reason,
          }
        : null;

      // Provide a durable history snapshot for prompt construction so persona switches share memory.
      // When rolling-summary compaction is enabled and a summary exists, trim history to:
      // - keep the configured head messages verbatim
      // - keep only messages after `summaryUptoTimestamp` (unsummarized tail)
      if (conversationContext) {
        const excludeRoles = new Set<MessageRole>([MessageRole.ERROR, MessageRole.THOUGHT]);
        const useTrimmedHistory =
          rollingSummaryEnabled &&
          typeof rollingSummaryResult?.summaryUptoTimestamp === 'number' &&
          rollingSummaryText.length > 0;

        const rawHistory = useTrimmedHistory
          ? (conversationContext.getAllMessages() as any[])
          : (conversationContext.getHistory(undefined, [MessageRole.ERROR, MessageRole.THOUGHT]) as any[]);

        let historyForPrompt = rawHistory.filter((m) => m && !excludeRoles.has(m.role)) as any[];

        const last = historyForPrompt[historyForPrompt.length - 1];
        if (last?.role === MessageRole.USER) {
          const content = typeof last.content === 'string' ? last.content.trim() : '';
          const inbound =
            gmiTurnInput.type === GMIInteractionType.TEXT && typeof gmiTurnInput.content === 'string'
              ? gmiTurnInput.content.trim()
              : gmiTurnInput.type === GMIInteractionType.MULTIMODAL_CONTENT
                ? JSON.stringify(gmiTurnInput.content).trim()
                : '';
          if (content && inbound && content === inbound) {
            historyForPrompt = historyForPrompt.slice(0, -1);
          }
        }

        if (useTrimmedHistory) {
          const headCount = Math.max(0, rollingSummaryConfigForTurn?.headMessagesToKeep ?? 0);
          const head = historyForPrompt.slice(0, Math.min(headCount, historyForPrompt.length));
          const afterSummary = historyForPrompt.filter((m: any) => m && m.timestamp > (rollingSummaryResult as any).summaryUptoTimestamp);
          const merged: any[] = [];
          const seen = new Set<string>();
          for (const msg of [...head, ...afterSummary]) {
            const id = typeof msg?.id === 'string' ? msg.id : '';
            if (!id || seen.has(id)) continue;
            seen.add(id);
            merged.push(msg);
          }
          historyForPrompt = merged;
        }

        turnMetadata.conversationHistoryForPrompt = historyForPrompt as any[];
      }

      // Persist any compaction/router metadata updates prior to the main LLM call.
      if (this.config.enableConversationalPersistence && conversationContext) {
        try {
          await this.dependencies.conversationManager.saveConversation(conversationContext);
        } catch (error: any) {
          console.warn(
            `AgentOSOrchestrator (Stream: ${agentOSStreamId}): Failed to persist conversation metadata updates.`,
            error,
          );
        }
      }

      // Best-effort: persist structured rolling memory (`memory_json`) to an external store for retrieval.
      if (
        rollingSummaryEnabled &&
        rollingSummaryResult?.didCompact &&
        typeof rollingSummaryResult.summaryText === 'string' &&
        this.dependencies.rollingSummaryMemorySink &&
        Boolean(longTermMemoryPolicy?.enabled) &&
        hasAnyLongTermMemoryScope(longTermMemoryPolicy ?? DEFAULT_LONG_TERM_MEMORY_POLICY)
      ) {
        const update: RollingSummaryMemoryUpdate = {
          userId: activeStreamCtx.userId,
          organizationId: organizationIdForMemory,
          sessionId: activeStreamCtx.sessionId,
          conversationId: activeStreamCtx.conversationId,
          personaId: activePersonaId,
          mode: modeForRouting,
          profileId: rollingSummaryProfileId,
          memoryPolicy: longTermMemoryPolicy ?? undefined,
          summaryText: rollingSummaryResult.summaryText,
          summaryJson: rollingSummaryResult.summaryJson ?? null,
          summaryUptoTimestamp: rollingSummaryResult.summaryUptoTimestamp ?? null,
          summaryUpdatedAt: rollingSummaryResult.summaryUpdatedAt ?? null,
        };
        void this.dependencies.rollingSummaryMemorySink
          .upsertRollingSummaryMemory(update)
          .catch((error: any) => {
            console.warn(
              `AgentOSOrchestrator (Stream: ${agentOSStreamId}): Rolling summary sink failed (continuing).`,
              error,
            );
          });
      }

      // Emit routing + memory metadata as a first-class chunk for clients.
      await this.pushChunkToStream(
        agentOSStreamId,
        AgentOSResponseChunkType.METADATA_UPDATE,
        gmiInstanceIdForChunks,
        activePersonaId,
        false,
        {
          updates: {
            promptProfile: promptProfileSelection,
            organizationId: organizationIdForMemory ?? null,
            longTermMemoryPolicy,
            rollingSummary: rollingSummaryResult
              ? {
                  profileId: rollingSummaryProfileId,
                  enabled: rollingSummaryResult.enabled,
                  didCompact: rollingSummaryResult.didCompact,
                  summaryText: rollingSummaryResult.summaryText,
                  summaryJson: rollingSummaryResult.summaryJson,
                  summaryUptoTimestamp: rollingSummaryResult.summaryUptoTimestamp,
                  summaryUpdatedAt: rollingSummaryResult.summaryUpdatedAt,
                  reason: rollingSummaryResult.reason,
                }
              : null,
          },
        },
      );
      
      const gmiStreamIterator = gmi.processTurnStream(gmiTurnInput);
      let finalGMIOutputFromStream: GMIOutput | undefined; 

      let iteratorResult = await gmiStreamIterator.next();
      while (!iteratorResult.done) {
        const gmiChunk = iteratorResult.value as GMIOutputChunk;
        await this.transformAndPushGMIChunk(agentOSStreamId, activeStreamCtx, gmiChunk);
        
        if (gmiChunk.type === GMIOutputChunkType.TOOL_CALL_REQUEST) {
          console.log(`AgentOSOrchestrator (Stream: ${agentOSStreamId}): GMI requested tool(s). Pausing internal GMI processing for this turn.`);
          return; 
        }
        iteratorResult = await gmiStreamIterator.next();
      }
      finalGMIOutputFromStream = iteratorResult.value as GMIOutput | undefined;

      if (this.config.enableConversationalPersistence && conversationContext) {
      if (finalGMIOutputFromStream?.responseText) {
        const assistantTextMessage: Omit<ConversationMessage, 'id' | 'timestamp'> = {
          role: MessageRole.ASSISTANT,
          content: finalGMIOutputFromStream.responseText,
          metadata: { agentPersonaId: activePersonaId },
        };
        conversationContext.addMessage(assistantTextMessage);
      }
      if (finalGMIOutputFromStream?.toolCalls) {
        const assistantToolMessage: Omit<ConversationMessage, 'id' | 'timestamp'> = {
          role: MessageRole.ASSISTANT,
          content: null,
          tool_calls: finalGMIOutputFromStream.toolCalls,
          metadata: { agentPersonaId: activePersonaId },
        };
        conversationContext.addMessage(assistantToolMessage);
      }
        await this.dependencies.conversationManager.saveConversation(conversationContext);
      }

      const personaDef = gmi.getPersona();
      
      if (!finalGMIOutputFromStream) {
        console.warn(`AgentOSOrchestrator (Stream: ${agentOSStreamId}): GMI stream completed without an explicit final GMIOutput. Constructing a default final response.`);
        const lastMessage = activeStreamCtx.conversationContext.getLastMessage();
        finalGMIOutputFromStream = {
          isFinal: true,
          responseText: lastMessage?.role === MessageRole.ASSISTANT ? (lastMessage.content as string || "Processing concluded.") : "Processing concluded.",
        };
      }
      
      await this.pushChunkToStream(
        agentOSStreamId, AgentOSResponseChunkType.FINAL_RESPONSE,
        gmiInstanceIdForChunks, activePersonaId, true, 
        {
          finalResponseText: finalGMIOutputFromStream.responseText,
          finalToolCalls: finalGMIOutputFromStream.toolCalls,
          finalUiCommands: finalGMIOutputFromStream.uiCommands,
          audioOutput: finalGMIOutputFromStream.audioOutput,
          imageOutput: finalGMIOutputFromStream.imageOutput,
          usage: normalizeUsage(finalGMIOutputFromStream.usage),
          reasoningTrace: finalGMIOutputFromStream.reasoningTrace,
          error: finalGMIOutputFromStream.error,
          updatedConversationContext: conversationContext.toJSON(),
          activePersonaDetails: snapshotPersonaDetails(personaDef),
        }
      );
      
        // TS2554 fix: Pass only 1 or 2 arguments
      await this.dependencies.streamingManager.closeStream(agentOSStreamId, "Turn processing complete."); 

    } catch (error: any) {
      const gmiErr = createGMIErrorFromError(error, GMIErrorCode.GMI_PROCESSING_ERROR, { agentOSStreamId }, `Unhandled error in _processTurnInternal`);
      console.error(`AgentOSOrchestrator: Error during _processTurnInternal for stream ${agentOSStreamId}:`, gmiErr);
      const personaIdForCatch = activePersonaId! || input.selectedPersonaId || 'unknown_error_persona'; // activePersonaId should be set if try block started
      await this.pushErrorChunk(
        agentOSStreamId, personaIdForCatch, gmiInstanceIdForChunks,
        gmiErr.code, gmiErr.message, gmiErr.details
      );
      if (this.activeStreamContexts.has(agentOSStreamId)) {
        // TS2554 fix: Pass only 1 or 2 arguments
        await this.dependencies.streamingManager.closeStream(agentOSStreamId, `Error: ${gmiErr.message}`); 
      }
    } finally {
      this.activeStreamContexts.delete(agentOSStreamId);
      console.log(`AgentOSOrchestrator: Cleaned up context for AgentOS Stream ${agentOSStreamId}.`);
    }
  }
  
  /**
   * Handles the result of an externally executed tool, feeding it back into the active GMI instance.
   * It then processes the GMI's subsequent output (`GMIOutput`), streaming further responses or
   * new tool requests as needed.
   *
   * @public
   * @async
   * @param {StreamId} agentOSStreamId - The orchestrator-level stream ID for this interaction flow.
   * @param {string} toolCallId - The ID of the specific tool call (from `ToolCallRequest`) whose result is being provided.
   * @param {string} toolName - The name of the tool that was executed.
   * @param {any} toolOutput - The output data returned by the tool execution.
   * @param {boolean} isSuccess - A flag indicating whether the tool execution was successful.
   * @param {string} [errorMessage] - An optional error message if `isSuccess` is false.
   * @returns {Promise<void>} Resolves when the tool result has been processed and subsequent GMI output streamed.
   * @throws {GMIError} If the `agentOSStreamId` is not found or if a critical error occurs during GMI's handling of the tool result.
   */
  public async orchestrateToolResult(
    agentOSStreamId: StreamId,
    toolCallId: string,
    toolName: string,
    toolOutput: any,
    isSuccess: boolean,
    errorMessage?: string,
  ): Promise<void> {
    this.ensureInitialized();
    const streamContext = this.activeStreamContexts.get(agentOSStreamId);

    if (!streamContext) {
      const errMsg = `AgentOSOrchestrator: Cannot orchestrate tool result. No active stream context found for streamId: ${agentOSStreamId}. Tool: ${toolName}, CallID: ${toolCallId}`;
      console.error(errMsg);
      throw new GMIError(errMsg, GMIErrorCode.RESOURCE_NOT_FOUND, { agentOSStreamId, toolCallId, toolName });
    }

    const { gmi, userId, personaId, userApiKeys, conversationContext } = streamContext;
    const gmiInstanceIdForChunks = gmi.getGMIId();

    const toolResultPayload: ToolResultPayload = isSuccess
      ? { type: 'success', result: toolOutput }
      : { type: 'error', error: { code: 'EXTERNAL_TOOL_ERROR', message: errorMessage || `External tool '${toolName}' execution failed.` } };

    console.log(`AgentOSOrchestrator (Stream: ${agentOSStreamId}): Feeding tool result for GMI ${gmiInstanceIdForChunks}, Tool Call ID ${toolCallId} (${toolName}) back to GMI.`);

    try {
      await this.pushChunkToStream(
        agentOSStreamId, AgentOSResponseChunkType.TOOL_RESULT_EMISSION,
        gmiInstanceIdForChunks, personaId, false,
        { toolCallId, toolName, toolResult: toolOutput, isSuccess, errorMessage }
      );

      if (this.config.enableConversationalPersistence && conversationContext) {
        try {
          conversationContext.addMessage({
            role: MessageRole.TOOL,
            content: typeof toolOutput === 'string' ? toolOutput : JSON.stringify(toolOutput),
            tool_call_id: toolCallId,
            name: toolName,
            metadata: { agentPersonaId: personaId, source: 'agentos_tool_result', isSuccess },
          });
          await this.dependencies.conversationManager.saveConversation(conversationContext);
        } catch (persistError: any) {
          console.warn(
            `AgentOSOrchestrator (Stream: ${agentOSStreamId}): Failed to persist tool result to ConversationContext.`,
            persistError,
          );
        }
      }

      const gmiOutputAfterTool: GMIOutput = await gmi.handleToolResult(
        toolCallId, toolName, toolResultPayload, userId, userApiKeys || {}
      );
      
      await this.processGMIOutput(agentOSStreamId, streamContext, gmiOutputAfterTool, true);

      if (gmiOutputAfterTool.toolCalls && gmiOutputAfterTool.toolCalls.length > 0 && !gmiOutputAfterTool.isFinal) {
        await this.pushChunkToStream(
          agentOSStreamId, AgentOSResponseChunkType.TOOL_CALL_REQUEST,
          gmiInstanceIdForChunks, personaId, false,
          { toolCalls: gmiOutputAfterTool.toolCalls, rationale: gmiOutputAfterTool.responseText || "Agent requires further tool execution based on previous tool's result." }
        );
      } else if (gmiOutputAfterTool.isFinal) {
        console.log(`AgentOSOrchestrator (Stream: ${agentOSStreamId}): GMI interaction concluded as final after tool result processing.`);
      }

    } catch (error: any) {
      const gmiErr = createGMIErrorFromError(error, GMIErrorCode.TOOL_ERROR, { agentOSStreamId, toolCallId, toolName }, `Error processing tool result for '${toolName}'`);
      console.error(`AgentOSOrchestrator (Stream: ${agentOSStreamId}): Critical error during orchestrateToolResult:`, gmiErr);
      await this.pushErrorChunk(
        agentOSStreamId, personaId, gmiInstanceIdForChunks,
        gmiErr.code, gmiErr.message, gmiErr.details
      );
      if (this.activeStreamContexts.has(agentOSStreamId)) {
        // TS2554 fix: Pass only 1 or 2 arguments
        await this.dependencies.streamingManager.closeStream(agentOSStreamId, `Critical error during tool result processing: ${gmiErr.message}`);
        this.activeStreamContexts.delete(agentOSStreamId);
      }
    }
  }
  
  /**
   * Processes a direct `GMIOutput` object (typically the result of `gmi.handleToolResult` or the
   * final return value of `gmi.processTurnStream`). It transforms and pushes the relevant
   * parts of this `GMIOutput` as `AgentOSResponse` chunks to the client stream.
   *
   * @private
   * @async
   * @param {StreamId} agentOSStreamId - The ID of the AgentOS stream.
   * @param {ActiveStreamContext} streamContext - The context for the current active stream.
   * @param {GMIOutput} gmiOutput - The GMI output object to process.
   * @param {boolean} isContinuationAfterTool - Indicates if this output is a direct result of processing a tool.
   * @returns {Promise<void>}
   */
  private async processGMIOutput(
      agentOSStreamId: StreamId,
      streamContext: ActiveStreamContext,
      gmiOutput: GMIOutput,
      _isContinuationAfterTool: boolean
  ): Promise<void> {
      const { gmi, personaId, conversationContext } = streamContext;
      const gmiInstanceIdForChunks = gmi.getGMIId();

      if (gmiOutput.responseText) {
        await this.pushChunkToStream(
          agentOSStreamId, AgentOSResponseChunkType.TEXT_DELTA,
          gmiInstanceIdForChunks, personaId, false,
          { textDelta: gmiOutput.responseText }
        );
      }
      if (gmiOutput.uiCommands && gmiOutput.uiCommands.length > 0) {
        await this.pushChunkToStream(
          agentOSStreamId, AgentOSResponseChunkType.UI_COMMAND,
          gmiInstanceIdForChunks, personaId, false,
          { uiCommands: gmiOutput.uiCommands }
        );
      }
      if (gmiOutput.error) {
        await this.pushErrorChunk(
          agentOSStreamId, personaId, gmiInstanceIdForChunks,
          gmiOutput.error.code, gmiOutput.error.message, gmiOutput.error.details
        );
        if (this.activeStreamContexts.has(agentOSStreamId)) {
            // TS2554 fix: Pass only 1 or 2 arguments
          await this.dependencies.streamingManager.closeStream(agentOSStreamId, `GMI reported an error: ${gmiOutput.error.message}`);
          this.activeStreamContexts.delete(agentOSStreamId);
        }
        return;
      }

      if (gmiOutput.isFinal && (!gmiOutput.toolCalls || gmiOutput.toolCalls.length === 0)) {
        if (this.config.enableConversationalPersistence && conversationContext) {
          try {
            if (gmiOutput.responseText) {
        const assistantFinalMessage: Omit<ConversationMessage, 'id' | 'timestamp'> = {
          role: MessageRole.ASSISTANT,
          content: gmiOutput.responseText,
          metadata: { agentPersonaId: personaId },
        };
        conversationContext.addMessage(assistantFinalMessage);
            }
            await this.dependencies.conversationManager.saveConversation(conversationContext);
            console.log(`AgentOSOrchestrator (Stream: ${agentOSStreamId}): Conversation context ${conversationContext.sessionId} saved.`);
          } catch (saveError: any) {
            console.error(`AgentOSOrchestrator (Stream: ${agentOSStreamId}): Failed to save conversation context ${conversationContext.sessionId}: ${saveError.message}`, saveError);
          }
        }
        const personaDef = gmi.getPersona();
        await this.pushChunkToStream(
          agentOSStreamId, AgentOSResponseChunkType.FINAL_RESPONSE,
          gmiInstanceIdForChunks, personaId, true,
          {
            finalResponseText: gmiOutput.responseText,
            finalToolCalls: gmiOutput.toolCalls,
            finalUiCommands: gmiOutput.uiCommands,
            audioOutput: gmiOutput.audioOutput,
            imageOutput: gmiOutput.imageOutput,
            usage: normalizeUsage(gmiOutput.usage),
            reasoningTrace: gmiOutput.reasoningTrace,
            error: gmiOutput.error,
            updatedConversationContext: conversationContext.toJSON(),
      activePersonaDetails: snapshotPersonaDetails(personaDef),
          }
        );
        if (this.activeStreamContexts.has(agentOSStreamId)) {
            // TS2554 fix: Pass only 1 or 2 arguments
          await this.dependencies.streamingManager.closeStream(agentOSStreamId, "Interaction processing complete, final response sent.");
          this.activeStreamContexts.delete(agentOSStreamId);
          console.log(`AgentOSOrchestrator (Stream: ${agentOSStreamId}): Finalized and cleaned up stream context.`);
        }
      }
  }

  /**
   * Transforms a single `GMIOutputChunk` from the GMI's stream into one or more
   * `AgentOSResponse` chunks and pushes them to the client via the `StreamingManager`.
   *
   * @private
   * @async
   * @param {StreamId} agentOSStreamId - The ID of the AgentOS stream.
   * @param {ActiveStreamContext} streamContext - The context for the current active stream.
   * @param {GMIOutputChunk} gmiChunk - The chunk of output received from the GMI.
   * @returns {Promise<void>}
   */
  private async transformAndPushGMIChunk(
    agentOSStreamId: StreamId,
    streamContext: ActiveStreamContext,
    gmiChunk: GMIOutputChunk
  ): Promise<void> {
    const { gmi, personaId } = streamContext;
    const gmiInstanceIdForChunks = gmi.getGMIId();

    const isChunkFinal = gmiChunk.isFinal ?? false;

    switch (gmiChunk.type) {
      case GMIOutputChunkType.TEXT_DELTA:
        if (gmiChunk.content && typeof gmiChunk.content === 'string') {
          await this.pushChunkToStream( agentOSStreamId, AgentOSResponseChunkType.TEXT_DELTA,
            gmiInstanceIdForChunks, personaId, isChunkFinal, { textDelta: gmiChunk.content });
        }
        break;
      case GMIOutputChunkType.SYSTEM_MESSAGE: {
        if (gmiChunk.content && typeof gmiChunk.content === 'object' && gmiChunk.content !== null) {
          const progressData = gmiChunk.content as { message: string; progressPercentage?: number; statusCode?: string};
          await this.pushChunkToStream( agentOSStreamId, AgentOSResponseChunkType.SYSTEM_PROGRESS,
            gmiInstanceIdForChunks, personaId, isChunkFinal, progressData);
        }
        break;
      }
      case GMIOutputChunkType.TOOL_CALL_REQUEST: {
        if (gmiChunk.content && Array.isArray(gmiChunk.content) && gmiChunk.content.length > 0) {
          const toolCalls = gmiChunk.content as ToolCallRequest[];
          const rationale = (gmiChunk.metadata?.rationale as string) || "Agent is considering using tools.";
          await this.pushChunkToStream( agentOSStreamId, AgentOSResponseChunkType.TOOL_CALL_REQUEST,
            gmiInstanceIdForChunks, personaId, false, // Tool call requests are typically not final for the turn
            { toolCalls, rationale });
        }
        break;
      }
      case GMIOutputChunkType.UI_COMMAND: {
        if (gmiChunk.content && Array.isArray(gmiChunk.content)) {
          await this.pushChunkToStream( agentOSStreamId, AgentOSResponseChunkType.UI_COMMAND,
            gmiInstanceIdForChunks, personaId, isChunkFinal, { uiCommands: gmiChunk.content as UICommand[] });
        }
        break;
      }
      case GMIOutputChunkType.ERROR: {
        const errorContent = gmiChunk.content;
        const errorDetailsObj = gmiChunk.errorDetails || (typeof errorContent === 'object' && errorContent !== null ? errorContent : { messageFromContent: String(errorContent) });
        
        await this.pushErrorChunk(agentOSStreamId, personaId, gmiInstanceIdForChunks,
          (errorDetailsObj as {code?: GMIErrorCode | string})?.code || GMIErrorCode.GMI_PROCESSING_ERROR,
          (errorDetailsObj as {message?: string})?.message || String(errorContent) || 'An unspecified GMI error occurred.',
          errorDetailsObj);

        if (isChunkFinal && this.activeStreamContexts.has(agentOSStreamId)) {
            // TS2554 fix: Pass only 1 or 2 arguments
          await this.dependencies.streamingManager.closeStream(agentOSStreamId, `GMI stream reported a final error: ${(errorDetailsObj as {message?: string})?.message || String(errorContent)}`);
          this.activeStreamContexts.delete(agentOSStreamId);
        }
        break;
      }
      case GMIOutputChunkType.FINAL_RESPONSE_MARKER:
        console.log(`AgentOSOrchestrator (Stream: ${agentOSStreamId}): Received FINAL_RESPONSE_MARKER from GMI. isFinal on chunk: ${isChunkFinal}.`);
        break;
      case GMIOutputChunkType.USAGE_UPDATE:
        if (this.config.logToolCalls) {
          console.log(`AgentOSOrchestrator (Stream: ${agentOSStreamId}): Received USAGE_UPDATE from GMI:`, gmiChunk.content);
        }
        await this.pushChunkToStream(
          agentOSStreamId, AgentOSResponseChunkType.METADATA_UPDATE,
          gmiInstanceIdForChunks, personaId, isChunkFinal, 
          { updates: { usage: gmiChunk.content } }
        );
        break;
      case GMIOutputChunkType.REASONING_STATE_UPDATE:
        if (this.config.logToolCalls) { 
          console.log(`AgentOSOrchestrator (Stream: ${agentOSStreamId}): GMI Reasoning State Update:`, gmiChunk.content);
        }
        await this.pushChunkToStream(
          agentOSStreamId, AgentOSResponseChunkType.METADATA_UPDATE,
          gmiInstanceIdForChunks, personaId, isChunkFinal, 
          { updates: { reasoningState: gmiChunk.content } }
        );
        break;
      case GMIOutputChunkType.LATENCY_REPORT:
        if (this.config.logToolCalls) { 
          console.log(`AgentOSOrchestrator (Stream: ${agentOSStreamId}): GMI Latency Report:`, gmiChunk.content);
        }
        await this.pushChunkToStream(
          agentOSStreamId, AgentOSResponseChunkType.METADATA_UPDATE,
          gmiInstanceIdForChunks, personaId, isChunkFinal,
          { updates: { latencyReport: gmiChunk.content } }
        );
        break;
      default: {
        const exhaustiveCheck: never = gmiChunk.type;
        console.warn(`AgentOSOrchestrator (Stream: ${agentOSStreamId}): Encountered an unhandled GMIOutputChunkType: '${exhaustiveCheck as string}'. Chunk content:`, gmiChunk.content);
        break;
      }
    }
  }

  /**
   * Gracefully shuts down the AgentOSOrchestrator.
   * This involves notifying and closing any active streams managed by the associated `StreamingManager`
   * and clearing internal state. Dependencies like `GMIManager` are assumed to be shut down
   * by the main `AgentOS` service.
   *
   * @public
   * @async
   * @returns {Promise<void>} A promise that resolves when the shutdown process is complete.
   */
  public async shutdown(): Promise<void> {
    if (!this.initialized) {
      console.warn("AgentOSOrchestrator: Shutdown called but orchestrator was not initialized or already shut down.");
      return;
    }
    console.log(`AgentOSOrchestrator: Initiating shutdown. Closing ${this.activeStreamContexts.size} active stream contexts...`);
    
    const streamClosePromises: Promise<void>[] = [];
    for (const streamId of this.activeStreamContexts.keys()) {
      console.log(`AgentOSOrchestrator: Requesting closure of stream ${streamId} due to orchestrator shutdown.`);
      streamClosePromises.push(
        // TS2554 fix: Pass only 1 or 2 arguments
        this.dependencies.streamingManager.closeStream(streamId, "AgentOS Orchestrator is shutting down.")
          .catch((e: any) => console.error(`AgentOSOrchestrator: Error closing stream ${streamId} during shutdown: ${e.message}`))
      );
    }
    
    try {
      await Promise.allSettled(streamClosePromises);
    } catch (e: any) {
      console.error("AgentOSOrchestrator: Errors occurred while closing active streams during shutdown:", e);
    }

    this.activeStreamContexts.clear();
    this.initialized = false;
    console.log("AgentOSOrchestrator: Shutdown complete. All active stream contexts cleared.");
  }
}
