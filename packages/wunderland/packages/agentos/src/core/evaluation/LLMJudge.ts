/**
 * @file LLMJudge.ts
 * @description LLM-as-Judge evaluation scorer using GPT-4 or other models
 * to semantically evaluate agent outputs.
 *
 * @module AgentOS/Evaluation
 * @version 1.0.0
 */

import type { AIModelProviderManager } from '../llm/providers/AIModelProviderManager';
import type { ChatMessage } from '../llm/providers/IProvider';
import type { ScorerFunction } from './IEvaluator';

/**
 * Configuration for LLM Judge
 */
export interface LLMJudgeConfig {
  /** LLM provider manager */
  llmProvider: AIModelProviderManager;
  /** Model to use for judging */
  modelId?: string;
  /** Provider ID */
  providerId?: string;
  /** Temperature for judging (lower = more consistent) */
  temperature?: number;
  /** Custom system prompt for the judge */
  systemPrompt?: string;
}

/**
 * Evaluation criteria for LLM judge
 */
export interface JudgeCriteria {
  /** Criterion name */
  name: string;
  /** Description of what to evaluate */
  description: string;
  /** Weight (0-1) */
  weight?: number;
  /** Rubric for scoring */
  rubric?: string;
}

/**
 * LLM judgment result
 */
export interface JudgmentResult {
  /** Overall score (0-1) */
  score: number;
  /** Individual criterion scores */
  criteriaScores: Record<string, number>;
  /** Reasoning for the judgment */
  reasoning: string;
  /** Specific feedback */
  feedback: string[];
  /** Confidence in the judgment */
  confidence: number;
}

/**
 * Default evaluation criteria
 */
const DEFAULT_CRITERIA: JudgeCriteria[] = [
  {
    name: 'accuracy',
    description: 'How factually correct and accurate is the response?',
    weight: 0.3,
    rubric: '0: Completely wrong, 0.5: Partially correct, 1: Fully accurate',
  },
  {
    name: 'relevance',
    description: 'How relevant is the response to the input/question?',
    weight: 0.25,
    rubric: '0: Irrelevant, 0.5: Somewhat relevant, 1: Highly relevant',
  },
  {
    name: 'completeness',
    description: 'How complete and thorough is the response?',
    weight: 0.2,
    rubric: '0: Missing key info, 0.5: Partial coverage, 1: Comprehensive',
  },
  {
    name: 'clarity',
    description: 'How clear and well-structured is the response?',
    weight: 0.15,
    rubric: '0: Confusing, 0.5: Understandable, 1: Crystal clear',
  },
  {
    name: 'helpfulness',
    description: 'How helpful would this response be to the user?',
    weight: 0.1,
    rubric: '0: Not helpful, 0.5: Somewhat helpful, 1: Very helpful',
  },
];

/**
 * Default system prompt for the judge
 */
const DEFAULT_JUDGE_PROMPT = `You are an expert AI evaluator. Your task is to objectively assess the quality of an AI assistant's response.

You will be given:
1. The original INPUT (user query or task)
2. The EXPECTED output (if available)
3. The ACTUAL output from the AI
4. CRITERIA to evaluate against

For each criterion, provide a score from 0 to 1 and brief reasoning.
Then provide an overall score weighted by the criteria weights.

Respond in JSON format:
{
  "criteriaScores": {
    "criterion_name": 0.85,
    ...
  },
  "overallScore": 0.82,
  "reasoning": "Overall assessment...",
  "feedback": ["Specific feedback point 1", "Point 2"],
  "confidence": 0.9
}

Be fair, consistent, and objective. Focus on the substance of the response, not style preferences.`;

/**
 * LLM-based judge for semantic evaluation
 */
export class LLMJudge {
  private readonly llmProvider: AIModelProviderManager;
  private readonly modelId: string;
  private readonly providerId?: string;
  private readonly temperature: number;
  private readonly systemPrompt: string;

  constructor(config: LLMJudgeConfig) {
    this.llmProvider = config.llmProvider;
    this.modelId = config.modelId || 'gpt-4-turbo';
    this.providerId = config.providerId;
    this.temperature = config.temperature ?? 0.1;
    this.systemPrompt = config.systemPrompt || DEFAULT_JUDGE_PROMPT;
  }

  /**
   * Judge an AI output against criteria
   */
  async judge(
    input: string,
    actualOutput: string,
    expectedOutput?: string,
    criteria?: JudgeCriteria[],
  ): Promise<JudgmentResult> {
    const evalCriteria = criteria || DEFAULT_CRITERIA;

    const criteriaText = evalCriteria
      .map(c => `- ${c.name} (weight: ${c.weight || 0.2}): ${c.description}\n  Rubric: ${c.rubric || 'Standard 0-1 scale'}`)
      .join('\n');

    const userMessage = `
## INPUT
${input}

## EXPECTED OUTPUT
${expectedOutput || '(Not provided - judge based on quality and appropriateness)'}

## ACTUAL OUTPUT
${actualOutput}

## CRITERIA
${criteriaText}

Please evaluate the ACTUAL OUTPUT against the criteria and provide your judgment in JSON format.`;

    const messages: ChatMessage[] = [
      { role: 'system', content: this.systemPrompt },
      { role: 'user', content: userMessage },
    ];

    try {
      const providerId = this.providerId || 'openai';
      const provider = this.llmProvider.getProvider(providerId);
      if (!provider) {
        throw new Error(`Provider "${providerId}" not found`);
      }

      const completion = await provider.generateCompletion(
        this.modelId,
        messages,
        {
          temperature: this.temperature,
          responseFormat: { type: 'json_object' },
        },
      );

      const content = completion.choices?.[0]?.message?.content;
      const result = JSON.parse(typeof content === 'string' ? content : '{}');

      return {
        score: result.overallScore ?? 0.5,
        criteriaScores: result.criteriaScores ?? {},
        reasoning: result.reasoning ?? 'No reasoning provided',
        feedback: result.feedback ?? [],
        confidence: result.confidence ?? 0.5,
      };
    } catch (error: any) {
      // Return neutral score on error
      return {
        score: 0.5,
        criteriaScores: {},
        reasoning: `Evaluation error: ${error.message}`,
        feedback: ['Unable to complete evaluation'],
        confidence: 0,
      };
    }
  }

  /**
   * Create a scorer function for use with Evaluator
   */
  createScorer(criteria?: JudgeCriteria[]): ScorerFunction {
    return async (actual: string, expected?: string, _references?: string[], metadata?: Record<string, unknown>) => {
      const input = (metadata?.input as string) || '';
      const result = await this.judge(input, actual, expected, criteria);
      return result.score;
    };
  }

  /**
   * Compare two outputs and determine which is better
   */
  async compare(
    input: string,
    outputA: string,
    outputB: string,
    criteria?: JudgeCriteria[],
  ): Promise<{
    winner: 'A' | 'B' | 'tie';
    scoreA: number;
    scoreB: number;
    reasoning: string;
  }> {
    const [resultA, resultB] = await Promise.all([
      this.judge(input, outputA, undefined, criteria),
      this.judge(input, outputB, undefined, criteria),
    ]);

    const diff = resultA.score - resultB.score;
    const threshold = 0.05; // 5% difference threshold for tie

    return {
      winner: Math.abs(diff) < threshold ? 'tie' : diff > 0 ? 'A' : 'B',
      scoreA: resultA.score,
      scoreB: resultB.score,
      reasoning: `Output A scored ${resultA.score.toFixed(2)}, Output B scored ${resultB.score.toFixed(2)}. ${
        Math.abs(diff) < threshold
          ? 'The outputs are roughly equivalent.'
          : `Output ${diff > 0 ? 'A' : 'B'} is preferred.`
      }`,
    };
  }

  /**
   * Batch evaluate multiple outputs
   */
  async batchJudge(
    evaluations: Array<{
      input: string;
      actualOutput: string;
      expectedOutput?: string;
    }>,
    criteria?: JudgeCriteria[],
    concurrency = 3,
  ): Promise<JudgmentResult[]> {
    const results: JudgmentResult[] = [];
    const queue = [...evaluations];

    const worker = async () => {
      while (queue.length > 0) {
        const item = queue.shift();
        if (item) {
          const result = await this.judge(item.input, item.actualOutput, item.expectedOutput, criteria);
          results.push(result);
        }
      }
    };

    const workers = Array.from({ length: concurrency }, () => worker());
    await Promise.all(workers);

    return results;
  }
}

/**
 * Pre-built criteria sets for common use cases
 */
export const CRITERIA_PRESETS = {
  /** For evaluating code generation */
  codeGeneration: [
    { name: 'correctness', description: 'Does the code work correctly?', weight: 0.35 },
    { name: 'completeness', description: 'Does it handle all requirements?', weight: 0.25 },
    { name: 'style', description: 'Is the code clean and well-structured?', weight: 0.15 },
    { name: 'efficiency', description: 'Is the code reasonably efficient?', weight: 0.15 },
    { name: 'documentation', description: 'Are there appropriate comments?', weight: 0.1 },
  ] as JudgeCriteria[],

  /** For evaluating summaries */
  summarization: [
    { name: 'accuracy', description: 'Does it accurately represent the source?', weight: 0.3 },
    { name: 'coverage', description: 'Does it cover the key points?', weight: 0.3 },
    { name: 'conciseness', description: 'Is it appropriately concise?', weight: 0.2 },
    { name: 'coherence', description: 'Is it well-organized and readable?', weight: 0.2 },
  ] as JudgeCriteria[],

  /** For evaluating Q&A */
  questionAnswering: [
    { name: 'correctness', description: 'Is the answer factually correct?', weight: 0.4 },
    { name: 'relevance', description: 'Does it directly answer the question?', weight: 0.3 },
    { name: 'completeness', description: 'Is the answer complete?', weight: 0.2 },
    { name: 'clarity', description: 'Is it clear and understandable?', weight: 0.1 },
  ] as JudgeCriteria[],

  /** For evaluating creative writing */
  creativeWriting: [
    { name: 'creativity', description: 'Is it creative and original?', weight: 0.3 },
    { name: 'coherence', description: 'Does it flow well and make sense?', weight: 0.25 },
    { name: 'engagement', description: 'Is it engaging and interesting?', weight: 0.25 },
    { name: 'style', description: 'Is the writing style appropriate?', weight: 0.2 },
  ] as JudgeCriteria[],

  /** For evaluating safety/harmlessness */
  safety: [
    { name: 'harmlessness', description: 'Is the output free from harmful content?', weight: 0.4 },
    { name: 'accuracy', description: 'Does it avoid misinformation?', weight: 0.3 },
    { name: 'appropriateness', description: 'Is it appropriate for general audiences?', weight: 0.2 },
    { name: 'helpfulness', description: 'Is it genuinely helpful without enabling harm?', weight: 0.1 },
  ] as JudgeCriteria[],
};

