"input","expected_output"
"Title: Attention Is All You Need
Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
Published: June 12, 2017. Conference: NeurIPS 2017.

Abstract: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs.","{""title"":""Attention Is All You Need"",""authors"":[""Ashish Vaswani"",""Noam Shazeer"",""Niki Parmar"",""Jakob Uszkoreit"",""Llion Jones"",""Aidan N. Gomez"",""Lukasz Kaiser"",""Illia Polosukhin""],""publicationDate"":""2017-06-12"",""source"":""NeurIPS 2017"",""keyFindings"":[""Transformer architecture based solely on attention mechanisms"",""28.4 BLEU on WMT 2014 English-to-German"",""41.8 BLEU on WMT 2014 English-to-French""],""keywords"":[""transformer"",""attention mechanism"",""machine translation""]}"
"Title: RAGAS: Automated Evaluation of Retrieval Augmented Generation
Authors: Shahul Es, Jithin James, Luis Espinosa-Anke, Steven Schockaert
Published: 2023-09-29. Journal: arXiv preprint arXiv:2309.15217.

Abstract: We introduce RAGAS, a framework for reference-free evaluation of RAG pipelines. RAGAS proposes three metrics — faithfulness, answer relevance, and context relevance — that can be computed automatically without reference to ground-truth answers. We demonstrate that RAGAS metrics correlate well with human judgments.

Limitations: Metrics rely on LLM calls which introduce cost and latency. Faithfulness detection may miss subtle hallucinations. Primarily evaluated on English-language datasets.","{""title"":""RAGAS: Automated Evaluation of Retrieval Augmented Generation"",""authors"":[""Shahul Es"",""Jithin James"",""Luis Espinosa-Anke"",""Steven Schockaert""],""publicationDate"":""2023-09-29"",""source"":""arXiv preprint arXiv:2309.15217"",""keyFindings"":[""Three automatic metrics: faithfulness, answer relevance, context relevance"",""Reference-free evaluation"",""Correlates with human judgments""],""keywords"":[""RAG evaluation"",""faithfulness"",""answer relevance""],""limitations"":[""LLM calls add cost/latency"",""May miss subtle hallucinations"",""English-only evaluation""]}"
"Title: Language Models are Few-Shot Learners
Authors: Tom B. Brown et al.
Published: 2020-05-28. Conference: NeurIPS 2020. Citations: 30,000+.

Abstract: We show that scaling up language models greatly improves task-agnostic, few-shot performance. We train GPT-3, an autoregressive language model with 175 billion parameters, and test its performance in the few-shot setting. GPT-3 achieves strong performance on many NLP benchmarks without any gradient updates or fine-tuning.","{""title"":""Language Models are Few-Shot Learners"",""authors"":[""Tom B. Brown et al.""],""publicationDate"":""2020-05-28"",""source"":""NeurIPS 2020"",""keyFindings"":[""Scaling improves few-shot performance"",""GPT-3 with 175B parameters"",""Strong NLP results without fine-tuning""],""keywords"":[""few-shot learning"",""GPT-3"",""scaling""],""citations"":30000}"
"Title: Constitutional AI: Harmlessness from AI Feedback
Authors: Yuntao Bai et al.
Published: 2022-12-15. Source: Anthropic Research.

Abstract: We experiment with training a harmless AI assistant through ""constitutional AI"" (CAI). The method uses a set of principles that the AI follows via self-critique and revision, then reinforcement learning from AI feedback (RLAIF). The result is helpful and harmless without extensive human labels.

Limitations: Constitution is hand-crafted. Depends on base model capability. Tested primarily with Claude.","{""title"":""Constitutional AI: Harmlessness from AI Feedback"",""authors"":[""Yuntao Bai et al.""],""publicationDate"":""2022-12-15"",""source"":""Anthropic Research"",""keyFindings"":[""Constitutional AI uses principles-based self-critique"",""RLAIF replaces human harmfulness labels"",""Helpful and harmless without extensive labeling""],""keywords"":[""constitutional AI"",""RLAIF"",""AI safety""],""limitations"":[""Hand-crafted constitution"",""Depends on base model capability"",""Tested with Claude only""]}"
"Title: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
Authors: Patrick Lewis et al.
Published: 2020-05-22. Conference: NeurIPS 2020.

Abstract: We explore retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. RAG models generate more specific, diverse and factual language than parametric-only baselines.","{""title"":""Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"",""authors"":[""Patrick Lewis et al.""],""publicationDate"":""2020-05-22"",""source"":""NeurIPS 2020"",""keyFindings"":[""RAG combines parametric and non-parametric memory"",""More specific, diverse and factual output""],""keywords"":[""RAG"",""retrieval-augmented generation"",""knowledge-intensive NLP""]}"
