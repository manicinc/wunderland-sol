"input","expected_output"
"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.","Most sequence-to-sequence models rely on complex recurrent or convolutional networks with separate encoder and decoder components."
"We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.","We introduce the Transformer, a simpler architecture that uses only attention and removes recurrence and convolutions."
"We introduce RAGAS, a framework for reference-free evaluation of RAG pipelines.","We present RAGAS, a framework for evaluating RAG pipelines without needing reference answers."
"RAGAS proposes three metrics — faithfulness, answer relevance, and context relevance — that can be computed automatically without reference to ground-truth answers.","RAGAS defines three automatic metrics: faithfulness, answer relevance, and context relevance, all computed without ground-truth references."
"We show that scaling up language models greatly improves task-agnostic, few-shot performance.","We show that larger language models can substantially improve task-agnostic performance in few-shot settings."
"We train GPT-3, an autoregressive language model with 175 billion parameters, and test its performance in the few-shot setting.","We train GPT-3, a 175B-parameter autoregressive language model, and evaluate how it performs with only a few examples."
"We experiment with training a harmless AI assistant through ""constitutional AI"" (CAI).","We investigate training a harmless AI assistant using Constitutional AI (CAI)."
"The method uses a set of principles that the AI follows via self-critique and revision, then reinforcement learning from AI feedback (RLAIF).","The approach starts from a set of principles, uses self-critique and revision, and then applies reinforcement learning from AI feedback (RLAIF)."
"We explore retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation.","We study retrieval-augmented generation (RAG), which combines a pre-trained model with external non-parametric memory to generate text."
"RAG models generate more specific, diverse and factual language than parametric-only baselines.","Compared with parametric-only baselines, RAG produces language that is more specific, more diverse, and more factual."
